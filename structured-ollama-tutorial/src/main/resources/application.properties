# Use a compact, instruction-following model
quarkus.langchain4j.ollama.chat-model.model-id=mistral:7b-instruct

# Local LLMs can take longer to respond, especially at first
quarkus.langchain4j.ollama.timeout=60s

# Optional: Debugging
# quarkus.langchain4j.ollama.log-requests=true
# quarkus.langchain4j.ollama.log-responses=true

quarkus.jackson.fail-on-empty-beans=false